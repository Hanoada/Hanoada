{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSrDo+VcBrkNrBmNVojWEx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanoada/Hanoada/blob/main/Capstone.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hti5sulGK5Xy",
        "outputId": "976afc86-1784-47cd-b9ce-8e7510d2df3f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/meteor_data.csv /content"
      ],
      "metadata": {
        "id": "y_v3H01cK7OV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PgPzAbgKMJx"
      },
      "outputs": [],
      "source": [
        "import geopy as geopy\n",
        "import keras.backend as k\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras import regularizers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.python.keras.metrics import BinaryAccuracy\n",
        "from yellowbrick import ClassBalance\n",
        "from yellowbrick.features import ParallelCoordinates\n",
        "import keras.layers.regularization\n",
        "from geopy.distance import geodesic\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# This line of code gathers the meteorite data set from the local environment and assembles it into a pandas DataFrame\n",
        "# called \"meteor_data\". This is the DataFrame used throughout the code.\n",
        "meteor_data = pd.read_csv(\"/content/drive/MyDrive/meteor_data.csv\")\n",
        "\n",
        "user_city = input(\"Please Input a City Name: \")\n",
        "user_state = input(\"Please Input a State Abbreviation (i.e., 'CA' for California): \")\n",
        "target_radius = int(input(\"Please Select a Search Radius: \"))\n",
        "\n",
        "# Use GeoPy GeoCoders to get the latitude and longitude coordinates for the user-input city and state.\n",
        "user_location = (user_city + ', ' + user_state)\n",
        "my_nom = geopy.geocoders.Nominatim(user_agent=\"myGeocoder\")\n",
        "location = my_nom.geocode(user_location)\n",
        "\n",
        "# Parameterize user input to use throughout program.\n",
        "user_lat = location.latitude\n",
        "user_lon = location.longitude\n",
        "target_point = (user_lat, user_lon)\n",
        "\n",
        "\n",
        "# ***START DETERMINESEARCH***\n",
        "# This function determines if there are at least 3 meteors within the initial search criteria. If not, it expands the\n",
        "# search years and radius until it has at least 3 data points. These changes are taken into account later in the\n",
        "# program.\n",
        "def determineSearch(meteor_data_ds, target_point_ds, target_radius_ds):\n",
        "    meteor_data_ds[\"in_target\"] = ((target_radius_ds >= meteor_data_ds.apply(\n",
        "        lambda row: geodesic(target_point_ds, (row[\"latitude\"], row[\"longitude\"])).miles, axis=1)) &\n",
        "                                   (meteor_data_ds[\"year\"] >= 1973) & (meteor_data_ds[\"year\"] <= 2023))\n",
        "    true_rows_ds = meteor_data_ds.loc[meteor_data_ds['in_target']]\n",
        "\n",
        "    while len(true_rows_ds) <= 3:\n",
        "        meteor_data_ds[\"in_target\"] = ((meteor_data_ds.apply(\n",
        "            lambda row: geodesic(target_point_ds, (row[\"latitude\"], row[\"longitude\"])).miles, axis=1) <=\n",
        "                                        target_radius_ds + 25) & (\n",
        "                                                   meteor_data_ds[\"year\"] >= min(meteor_data_ds[\"year\"])))\n",
        "        true_rows_ds = meteor_data_ds.loc[meteor_data_ds['in_target']]\n",
        "        target_radius_ds = target_radius_ds + 25\n",
        "\n",
        "    return meteor_data_ds, target_point_ds, target_radius_ds, true_rows_ds\n",
        "# ***END DETERMINESEARCH***\n",
        "\n",
        "\n",
        "# ***START SMOTESAMPLER***\n",
        "# This function is used to resample the extremely imbalanced data by creating synthetic data points. Depending on the\n",
        "# number of meteor impacts within the user's original radius, it the function uses a different sampling strategy. The\n",
        "# sampling strategies were created through trial-and-error to try and get higher accuracy in the model.\n",
        "def overSample(true_rows_os, x_os, y_os):\n",
        "    if len(true_rows_os) > 5:\n",
        "        over_sample = SMOTE(random_state=42, sampling_strategy=0.30)\n",
        "        X_resampled_os, y_resampled_os = over_sample.fit_resample(x_os, y_os)\n",
        "    elif len(true_rows_os) == 5:\n",
        "        over_sample = SMOTE(random_state=42, k_neighbors=4, sampling_strategy=0.40)\n",
        "        X_resampled_os, y_resampled_os = over_sample.fit_resample(x_os, y_os)\n",
        "    elif len(true_rows_os) == 4:\n",
        "        over_sample = SMOTE(random_state=42, k_neighbors=3, sampling_strategy=0.50)\n",
        "        X_resampled_os, y_resampled_os = over_sample.fit_resample(x_os, y_os)\n",
        "    else:\n",
        "        # Safeguard... If the function completely fails, the program should exit because the data will not be resampled,\n",
        "        # which will cause the program to exit in error.\n",
        "        X_resampled_os = None\n",
        "        y_resampled_os = None\n",
        "        print(\"NO METEORS WITHIN DATA RANGE\")\n",
        "        exit(0)\n",
        "\n",
        "    return X_resampled_os, y_resampled_os\n",
        "# ***END SMOTESAMPLER***\n",
        "\n",
        "# Call the \"determineSearch()\" function in the \"DetermineSearch.py\" file included with this program.\n",
        "meteor_data, target_point, target_radius2, true_rows = determineSearch(meteor_data, target_point, target_radius)\n",
        "\n",
        "# Create a function to determine the number of years spanned by the meteors within the user's search criteria.\n",
        "years_spanned = max(true_rows['year']) - min(true_rows['year'])\n",
        "\n",
        "# Create a list of feature names to use when using / splitting data and training the Neural Network.\n",
        "feature_names = ['latitude', 'longitude', 'year']\n",
        "\n",
        "# Load the meteor_data data set into a Pandas DataFrame object.\n",
        "X = meteor_data[feature_names]\n",
        "y = meteor_data['in_target']\n",
        "\n",
        "# Use \"overSample()\" function in the \"SMOTESampler.py\" file included with this program to balance the data\n",
        "# by creating synthetic test cases.\n",
        "X_resampled, y_resampled = overSample(true_rows, X, y)\n",
        "\n",
        "# Split the data set into training, validation, and test sets.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
        "X_train_new, X_val, y_train_new, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2,\n",
        "                                                          random_state=42)\n",
        "\n",
        "# Define the Neural Network architecture. I chose to include an input layer with 64 neurons, followed by a\n",
        "# hidden layers of 8 neurons and an output layer of one neuron.\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.05), input_shape=[3]),\n",
        "    keras.layers.Dropout(0.01),\n",
        "    keras.layers.Dense(16, activation='relu', kernel_regularizer=regularizers.l2(0.05)),\n",
        "    keras.layers.Dropout(0.01),\n",
        "    keras.layers.Dense(4, activation='relu', kernel_regularizer=regularizers.l2(0.05)),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model with binary cross-entropy loss and Adam optimizer. Binary cross-entropy calculates the\n",
        "# difference between the predicted output and true output and adjusts the model's biases to minimize the\n",
        "# difference. The \"Adam()\" optimizer is a stochastic gradient descent optimization algorithm used in training\n",
        "# deep neural networks. 0.001 is the learning rate, which determines the step size at which the optimizer\n",
        "# updates the parameters while training.\n",
        "# model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
        "              metrics=['accuracy', BinaryAccuracy()])\n",
        "\n",
        "# The test_input below is a NumPy array of the user-entered 'latitude' and 'longitude' as well as an entry for\n",
        "# each year from 2024 to 2074 (25 years).\n",
        "test_input = np.array([([user_lat, user_lon, 2024], [user_lat, user_lon, 2025], [user_lat, user_lon, 2026],\n",
        "                        [user_lat, user_lon, 2027], [user_lat, user_lon, 2028], [user_lat, user_lon, 2029],\n",
        "                        [user_lat, user_lon, 2030], [user_lat, user_lon, 2031], [user_lat, user_lon, 2032],\n",
        "                        [user_lat, user_lon, 2033], [user_lat, user_lon, 2034], [user_lat, user_lon, 2035],\n",
        "                        [user_lat, user_lon, 2036], [user_lat, user_lon, 2037], [user_lat, user_lon, 2038],\n",
        "                        [user_lat, user_lon, 2039], [user_lat, user_lon, 2040], [user_lat, user_lon, 2041],\n",
        "                        [user_lat, user_lon, 2042], [user_lat, user_lon, 2043], [user_lat, user_lon, 2044],\n",
        "                        [user_lat, user_lon, 2045], [user_lat, user_lon, 2046], [user_lat, user_lon, 2047],\n",
        "                        [user_lat, user_lon, 2048])])\n",
        "# This line of code reshapes the data to fit the values above.\n",
        "test_input = test_input.reshape((25, 3))\n",
        "\n",
        "# The history code trains the neural network model using the \"fit\" method in Keras. Epochs is the number of\n",
        "# times the model will iterate over the data set. 30% of the data is split for validation.\n",
        "history = model.fit(X_test, y_test, epochs=100, validation_split=0.3, shuffle=True, verbose=1)\n",
        "\n",
        "# Get the binary accuracy of the model. Used in the final prediction calculation. The epochs variable needs to\n",
        "# be set as a range because the neural network may or may not actually iterate 100 times.\n",
        "train_acc = history.history['binary_accuracy']\n",
        "val_acc = history.history['val_binary_accuracy']\n",
        "epochs = range(1, len(train_acc) + 1)\n",
        "\n",
        "# Plot the training and validation accuracy graph. The 'alpha' argument sets transparency level for the lines.\n",
        "plt.plot(epochs, train_acc, '-', color='blue', label='Training Accuracy', alpha=0.5)\n",
        "plt.plot(epochs, val_acc, '-', color='red', label='Validation Accuracy', alpha=0.5)\n",
        "plt.title('Training vs. Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate the neural network on the test data.\n",
        "test_results = model.evaluate(X_test, y_test)\n",
        "\n",
        "# Plot the training & validation loss values.\n",
        "plt.plot(history.history['loss'], color='blue', alpha=0.5, label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], color='red', alpha=0.5, label='Validation Loss')\n",
        "plt.title('Training vs. Validation Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training Loss', 'Validation Loss'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Make predictions on new data. This is pulling the test_input from above, which has 50 listed items, all of\n",
        "# which have the user's latitude and longitude locations, but each have a different year to predict over a 50\n",
        "# year timespan. Then, the predictions are averaged to get an overall percentage per year. This is used later\n",
        "# to calculate the percentage in the next 25 year.\n",
        "y_pred = model.predict(test_input, batch_size=25)\n",
        "average_year_percent = y_pred.sum() / 25\n",
        "\n",
        "# Calculate the probability percentage for 1 year. Used to calculate probability over 25 years.\n",
        "percent_predict = 1 - (pow((1 - (float(average_year_percent / 100))), 25))\n",
        "\n",
        "# Normalize the distance since the radius may have been changed to gather more data points.\n",
        "normalized_radius = target_radius / target_radius2\n",
        "\n",
        "# Normalize prediction with normalized radius and percentage. Used to account for radius change in case\n",
        "# the user's location doesn't have meteors within the range criteria. See DetermineSearch.py for more info.\n",
        "if target_radius2 > target_radius:\n",
        "    normalized_prediction = normalized_radius * percent_predict\n",
        "else:\n",
        "    normalized_prediction = percent_predict\n",
        "\n",
        "# Create class names for ParallelCoordinates visualization and instantiate visualizer.\n",
        "classes = ['in-target', 'not in-target']\n",
        "visualizer = ParallelCoordinates(classes=classes, features=feature_names, normalize='standard',\n",
        "                                 shuffle=False, sample=0.5)\n",
        "visualizer_CB = ClassBalance(labels=[\"No Impact\", \"Impact\"])\n",
        "visualizer_CB.fit(y_train, y_test)\n",
        "visualizer_CB.show()\n",
        "\n",
        "# Create parameter for final prediction. This puts the prediction in the correct format for GUI output, which\n",
        "# will use a string formatter to output as a percentage to the user.\n",
        "final_prediction = 100 * float(normalized_prediction)\n",
        "\n",
        "print(f\"The predicted probability of a meteor strike within the next 25 years at {user_city}, {user_state} is: \\x1b[31m\\\"{final_prediction:.2f}%\\\"\\x1b[0m\")"
      ]
    }
  ]
}